name: Test Suite

on:
  push:
    branches: [main]
  pull_request:
    branches:
      - main
      - 'release/**'

env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '20'

jobs:
  # Determine which test suites to run based on changed files
  changes:
    name: Detect Changes
    runs-on: ubuntu-latest
    outputs:
      backend: ${{ steps.filter.outputs.backend }}
      frontend: ${{ steps.filter.outputs.frontend }}
      api: ${{ steps.filter.outputs.api }}
      export: ${{ steps.filter.outputs.export }}
      streaming: ${{ steps.filter.outputs.streaming }}
      config: ${{ steps.filter.outputs.config }}
      slides: ${{ steps.filter.outputs.slides }}
      genie: ${{ steps.filter.outputs.genie }}
    steps:
      - uses: actions/checkout@v4
      - uses: dorny/paths-filter@v3
        id: filter
        with:
          filters: |
            backend:
              - 'src/**'
              - 'tests/**'
              - 'pyproject.toml'
            frontend:
              - 'frontend/**'
            api:
              - 'src/api/**'
              - 'tests/integration/test_api_routes.py'
            export:
              - 'src/api/routes/export.py'
              - 'src/api/services/export*.py'
              - 'src/core/pptx_export.py'
              - 'tests/integration/test_export.py'
            streaming:
              - 'src/api/routes/chat.py'
              - 'src/api/services/chat*.py'
              - 'src/core/agent*.py'
              - 'tests/integration/test_streaming.py'
            config:
              - 'src/api/routes/settings/**'
              - 'src/services/config*.py'
              - 'src/database/**'
              - 'tests/integration/test_config_api.py'
            slides:
              - 'src/core/slide*.py'
              - 'src/api/routes/slides.py'
              - 'tests/integration/test_slide*.py'
            genie:
              - 'src/core/genie*.py'
              - 'src/tools/genie*.py'
              - 'tests/integration/test_genie*.py'

  # Fast unit tests - always run, gate for other tests
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    needs: changes
    if: needs.changes.outputs.backend == 'true'
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"

      - name: Run unit tests
        run: |
          pytest tests/unit -v --tb=short -n auto \
            --junitxml=test-results/unit-results.xml \
            --cov=src --cov-report=xml --cov-report=term-missing

      - name: Upload unit test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: unit-test-results
          path: test-results/

      - name: Upload coverage report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: coverage-report
          path: coverage.xml

  # Integration test: API Routes
  integration-api-routes:
    name: "Integration: API Routes"
    runs-on: ubuntu-latest
    needs: [changes, unit-tests]
    if: |
      always() &&
      (needs.unit-tests.result == 'success' || needs.unit-tests.result == 'skipped') &&
      (needs.changes.outputs.api == 'true' || needs.changes.outputs.backend == 'true')
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_USER: test
          POSTGRES_PASSWORD: test
          POSTGRES_DB: test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      - run: pip install -e ".[dev]"
      - name: Run API routes tests
        env:
          DATABASE_URL: postgresql://test:test@localhost:5432/test_db
          ENVIRONMENT: test
          DATABRICKS_HOST: https://test.cloud.databricks.com
          DATABRICKS_TOKEN: test-token
        run: |
          pytest tests/integration/test_api_routes.py -v --tb=short \
            --junitxml=test-results/api-routes-results.xml -m "not live"
      - uses: actions/upload-artifact@v4
        if: always()
        with:
          name: integration-api-routes-results
          path: test-results/

  # Integration test: Config API
  integration-config-api:
    name: "Integration: Config API"
    runs-on: ubuntu-latest
    needs: [changes, unit-tests]
    if: |
      always() &&
      (needs.unit-tests.result == 'success' || needs.unit-tests.result == 'skipped') &&
      (needs.changes.outputs.config == 'true' || needs.changes.outputs.backend == 'true')
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_USER: test
          POSTGRES_PASSWORD: test
          POSTGRES_DB: test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      - run: pip install -e ".[dev]"
      - name: Run config API tests
        env:
          DATABASE_URL: postgresql://test:test@localhost:5432/test_db
          ENVIRONMENT: test
          DATABRICKS_HOST: https://test.cloud.databricks.com
          DATABRICKS_TOKEN: test-token
        run: |
          pytest tests/integration/test_config_api.py -v --tb=short \
            --log-cli-level=WARNING -s \
            --junitxml=test-results/config-api-results.xml -m "not live"
      - uses: actions/upload-artifact@v4
        if: always()
        with:
          name: integration-config-api-results
          path: test-results/

  # Integration test: Export
  integration-export:
    name: "Integration: Export"
    runs-on: ubuntu-latest
    needs: [changes, unit-tests]
    if: |
      always() &&
      (needs.unit-tests.result == 'success' || needs.unit-tests.result == 'skipped') &&
      (needs.changes.outputs.export == 'true' || needs.changes.outputs.backend == 'true')
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_USER: test
          POSTGRES_PASSWORD: test
          POSTGRES_DB: test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      - run: pip install -e ".[dev]"
      - name: Run export tests
        env:
          DATABASE_URL: postgresql://test:test@localhost:5432/test_db
          ENVIRONMENT: test
          DATABRICKS_HOST: https://test.cloud.databricks.com
          DATABRICKS_TOKEN: test-token
        run: |
          pytest tests/integration/test_export.py -v --tb=short \
            --junitxml=test-results/export-results.xml -m "not live"
      - uses: actions/upload-artifact@v4
        if: always()
        with:
          name: integration-export-results
          path: test-results/

  # Integration test: Streaming
  integration-streaming:
    name: "Integration: Streaming"
    runs-on: ubuntu-latest
    needs: [changes, unit-tests]
    if: |
      always() &&
      (needs.unit-tests.result == 'success' || needs.unit-tests.result == 'skipped') &&
      (needs.changes.outputs.streaming == 'true' || needs.changes.outputs.backend == 'true')
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_USER: test
          POSTGRES_PASSWORD: test
          POSTGRES_DB: test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      - run: pip install -e ".[dev]"
      - name: Run streaming tests
        env:
          DATABASE_URL: postgresql://test:test@localhost:5432/test_db
          ENVIRONMENT: test
          DATABRICKS_HOST: https://test.cloud.databricks.com
          DATABRICKS_TOKEN: test-token
        run: |
          pytest tests/integration/test_streaming.py -v --tb=short \
            --junitxml=test-results/streaming-results.xml -m "not live"
      - uses: actions/upload-artifact@v4
        if: always()
        with:
          name: integration-streaming-results
          path: test-results/

  # Integration test: Slides
  integration-slides:
    name: "Integration: Slides"
    runs-on: ubuntu-latest
    needs: [changes, unit-tests]
    if: |
      always() &&
      (needs.unit-tests.result == 'success' || needs.unit-tests.result == 'skipped') &&
      (needs.changes.outputs.slides == 'true' || needs.changes.outputs.backend == 'true')
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_USER: test
          POSTGRES_PASSWORD: test
          POSTGRES_DB: test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      - run: pip install -e ".[dev]"
      - name: Run slide tests
        env:
          DATABASE_URL: postgresql://test:test@localhost:5432/test_db
          ENVIRONMENT: test
          DATABRICKS_HOST: https://test.cloud.databricks.com
          DATABRICKS_TOKEN: test-token
        run: |
          pytest tests/integration/test_slide_deck_integration.py tests/integration/test_slide_replacement_flow.py \
            -v --tb=short --junitxml=test-results/slides-results.xml -m "not live"
      - uses: actions/upload-artifact@v4
        if: always()
        with:
          name: integration-slides-results
          path: test-results/

  # Integration test: Genie
  integration-genie:
    name: "Integration: Genie"
    runs-on: ubuntu-latest
    needs: [changes, unit-tests]
    if: |
      always() &&
      (needs.unit-tests.result == 'success' || needs.unit-tests.result == 'skipped') &&
      (needs.changes.outputs.genie == 'true' || needs.changes.outputs.backend == 'true')
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_USER: test
          POSTGRES_PASSWORD: test
          POSTGRES_DB: test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      - run: pip install -e ".[dev]"
      - name: Run Genie tests
        env:
          DATABASE_URL: postgresql://test:test@localhost:5432/test_db
          ENVIRONMENT: test
          DATABRICKS_HOST: https://test.cloud.databricks.com
          DATABRICKS_TOKEN: test-token
        run: |
          # Exit code 5 means no tests collected (all deselected) - treat as success
          # These files currently only contain 'live' tests requiring real Databricks
          pytest tests/integration/test_genie_integration.py tests/integration/test_client_integration.py \
            -v --tb=short --junitxml=test-results/genie-results.xml -m "not live" \
            || test $? -eq 5
      - uses: actions/upload-artifact@v4
        if: always()
        with:
          name: integration-genie-results
          path: test-results/

  # E2E tests with Playwright - run each test file in isolation
  e2e-tests:
    name: E2E - ${{ matrix.test }}
    runs-on: ubuntu-latest
    needs: [changes, unit-tests]
    if: |
      always() &&
      (needs.unit-tests.result == 'success' || needs.unit-tests.result == 'skipped') &&
      needs.changes.outputs.frontend == 'true'
    
    strategy:
      fail-fast: false
      matrix:
        test:
          - chat-ui
          - deck-integrity
          - deck-prompts-integration
          - deck-prompts-ui
          - export-ui
          - help-ui
          - history-integration
          - history-ui
          - profile-integration
          - profile-ui
          - slide-operations-ui
          - slide-styles-integration
          - slide-styles-ui
    
    # Use PostgreSQL for E2E tests - matches production and handles concurrency well
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_USER: test
          POSTGRES_PASSWORD: test
          POSTGRES_DB: test_db
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
      - uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json

      - name: Install frontend dependencies
        working-directory: frontend
        run: npm ci

      - name: Install Playwright browsers
        working-directory: frontend
        run: npx playwright install --with-deps chromium

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install backend dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"

      - name: Seed database with default data
        env:
          DATABASE_URL: postgresql://test:test@localhost:5432/test_db
          ENVIRONMENT: test
          DATABRICKS_HOST: https://test.databricks.com
          DATABRICKS_TOKEN: test-token
        run: |
          echo "Initializing database tables and seeding default data..."
          python -c "
          from src.core.database import init_db, get_db_session
          from src.core.init_default_profile import seed_defaults
          from src.database.models import ConfigProfile, ConfigAIInfra, ConfigPrompts, SlideStyleLibrary
          
          # Create tables
          init_db()
          print('Tables created')
          
          # Seed deck prompts and slide styles (no Genie config required)
          seed_defaults()
          print('Default deck prompts and slide styles seeded')
          
          # Create a minimal default profile for E2E tests
          with get_db_session() as db:
              # Check if profile already exists
              existing = db.query(ConfigProfile).filter_by(name='default').first()
              if not existing:
                  # Get the first slide style
                  style = db.query(SlideStyleLibrary).first()
                  style_id = style.id if style else None
                  
                  # Create profile
                  profile = ConfigProfile(
                      name='default',
                      description='Default test profile',
                      is_default=True
                  )
                  db.add(profile)
                  db.flush()
                  
                  # Create AI infra config
                  ai_infra = ConfigAIInfra(
                      profile_id=profile.id,
                      llm_endpoint='databricks-claude-sonnet',
                      llm_temperature=0.7,
                      llm_max_tokens=4096
                  )
                  db.add(ai_infra)
                  
                  # Create prompts config
                  prompts = ConfigPrompts(
                      profile_id=profile.id,
                      selected_slide_style_id=style_id,
                      system_prompt='You are a helpful assistant.',
                      slide_editing_instructions='Edit slides as requested.'
                  )
                  db.add(prompts)
                  
                  db.commit()
                  print(f'Created default profile (id={profile.id})')
              else:
                  print('Default profile already exists')
          "

      - name: Verify database seeding
        env:
          DATABASE_URL: postgresql://test:test@localhost:5432/test_db
          ENVIRONMENT: test
          DATABRICKS_HOST: https://test.databricks.com
          DATABRICKS_TOKEN: test-token
        run: |
          echo "Verifying database state..."
          python -c "
          from src.core.database import get_db_session
          from src.database.models import ConfigProfile, SlideStyleLibrary, SlideDeckPromptLibrary
          
          with get_db_session() as db:
              profiles = db.query(ConfigProfile).all()
              styles = db.query(SlideStyleLibrary).all()
              prompts = db.query(SlideDeckPromptLibrary).all()
              
              print(f'Profiles: {len(profiles)}')
              for p in profiles:
                  print(f'  - {p.name} (id={p.id}, is_default={p.is_default})')
              
              print(f'Slide Styles: {len(styles)}')
              for s in styles:
                  print(f'  - {s.name} (id={s.id})')
              
              print(f'Deck Prompts: {len(prompts)}')
              for d in prompts:
                  print(f'  - {d.name} (id={d.id})')
              
              if len(profiles) == 0:
                  raise Exception('No profiles found - seeding failed!')
              if len(styles) == 0:
                  raise Exception('No slide styles found - seeding failed!')
          "

      - name: Start backend server
        run: |
          DATABASE_URL="postgresql://test:test@localhost:5432/test_db" \
          ENVIRONMENT="test" \
          DATABRICKS_HOST="https://test.databricks.com" \
          DATABRICKS_TOKEN="test-token" \
          python -m uvicorn src.api.main:app --host 0.0.0.0 --port 8000 &
          for i in {1..30}; do
            if curl -s http://localhost:8000/api/health > /dev/null 2>&1; then
              echo "Backend is ready"
              break
            fi
            if [ $i -eq 30 ]; then
              echo "Backend failed to start"
              exit 1
            fi
            echo "Waiting for backend... ($i/30)"
            sleep 1
          done

      - name: Verify API endpoints work
        run: |
          echo "Testing API endpoints..."
          echo "--- Health check ---"
          curl -s http://localhost:8000/api/health | head -c 200
          echo ""
          echo "--- Profiles API ---"
          curl -s --max-time 10 http://localhost:8000/api/settings/profiles | head -c 500
          echo ""
          echo "--- Deck Prompts API ---"
          curl -s --max-time 10 http://localhost:8000/api/settings/deck-prompts | head -c 500
          echo ""
          echo "API verification complete"

      - name: Run E2E tests - ${{ matrix.test }}
        working-directory: frontend
        env:
          CI: true
          VITE_API_URL: http://127.0.0.1:8000
        run: |
          echo "=== Pre-flight backend check ==="
          echo "Checking 127.0.0.1:8000..."
          curl -s --max-time 5 http://127.0.0.1:8000/api/health || echo "127.0.0.1:8000 FAILED"
          echo ""
          echo "Checking backend process..."
          ps aux | grep uvicorn | grep -v grep || echo "No uvicorn process found"
          echo "=== End pre-flight check ==="
          echo ""
          echo "Running test: ${{ matrix.test }}.spec.ts"
          npx playwright test tests/e2e/${{ matrix.test }}.spec.ts --project=chromium --workers=1

      - name: Upload Playwright report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: playwright-report-${{ matrix.test }}
          path: frontend/playwright-report/

      - name: Upload Playwright test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: playwright-results-${{ matrix.test }}
          path: frontend/test-results/

  # Summary job - runs after all tests
  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs:
      - unit-tests
      - integration-api-routes
      - integration-config-api
      - integration-export
      - integration-streaming
      - integration-slides
      - integration-genie
      - e2e-tests
    if: always()
    steps:
      - name: Check test results
        run: |
          echo "Unit Tests: ${{ needs.unit-tests.result }}"
          echo "API Routes: ${{ needs.integration-api-routes.result }}"
          echo "Config API: ${{ needs.integration-config-api.result }}"
          echo "Export: ${{ needs.integration-export.result }}"
          echo "Streaming: ${{ needs.integration-streaming.result }}"
          echo "Slides: ${{ needs.integration-slides.result }}"
          echo "Genie: ${{ needs.integration-genie.result }}"
          echo "E2E: ${{ needs.e2e-tests.result }}"

          # Fail if any required job failed (not skipped)
          failed=false
          for result in "${{ needs.unit-tests.result }}" \
                        "${{ needs.integration-api-routes.result }}" \
                        "${{ needs.integration-config-api.result }}" \
                        "${{ needs.integration-export.result }}" \
                        "${{ needs.integration-streaming.result }}" \
                        "${{ needs.integration-slides.result }}" \
                        "${{ needs.integration-genie.result }}" \
                        "${{ needs.e2e-tests.result }}"; do
            if [ "$result" == "failure" ]; then
              failed=true
              break
            fi
          done

          if [ "$failed" == "true" ]; then
            echo "One or more test jobs failed"
            exit 1
          fi
          echo "All tests passed or were skipped!"
